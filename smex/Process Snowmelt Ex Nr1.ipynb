{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ipyparallel as p\n",
    "import os\n",
    "import ntpath\n",
    "import ProgressBar as pb\n",
    "from pymail import alert\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exname = 'smex7_test'\n",
    "method = '7'\n",
    "site = 'Nr1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Volumes/data/RHESSys_out/Nwt/'+exname+'/*_basin.daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = pd.read_pickle('./data/params_%s_smex_method%s.pcl'%(site,method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = pd.DataFrame({'basin_daily':files})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIDX(fl):\n",
    "    return int(fl.split('_')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files['idx'] = files.basin_daily.map(getIDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = pd.merge(params,files, on= 'idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = p.Client()\n",
    "view = c.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import os\n",
    "\n",
    "def wateryear(index):\n",
    "    year = index.year\n",
    "    strt = pd.datetime(int(year),10,1) # start of the next water year+1\n",
    "    \n",
    "    if index<strt:\n",
    "        wyear = year\n",
    "    elif index>=strt: \n",
    "        wyear = year+1\n",
    "\n",
    "    return wyear\n",
    "\n",
    "def readRHESSysBasin(filename):\n",
    "        \n",
    "    if os.path.isfile(filename) == False:\n",
    "        print 'Input file does not exist'\n",
    "    else:\n",
    "        bd = filename # basin output path\n",
    "\n",
    "        bddat = pd.read_table(bd,delim_whitespace=True,\n",
    "                               header='infer', parse_dates={'foo':[2,1,0]}, index_col='foo')\n",
    "        bddat.reset_index(inplace=True)\n",
    "        #bddat.index = pd.DatetimeIndex(bddat['foo'])\n",
    "\n",
    "        data = bddat\n",
    "        del data['foo']\n",
    "        \n",
    "        #data['wateryear'] = data.index.map(wateryear) # calculate the water year\n",
    "        \n",
    "        # calculate accumulated precip and snowfall by water year\n",
    "        \n",
    "        data['ET'] = data.evap+data.trans\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readRHESSysBasin(filename):\n",
    "        \n",
    "    if os.path.isfile(filename) == False:\n",
    "        print 'Input file does not exist'\n",
    "    else:\n",
    "        bd = filename # basin output path\n",
    "\n",
    "        bddat = pd.read_table(bd,delim_whitespace=True,\n",
    "                               header='infer', parse_dates={'foo':[2,1,0]}, index_col='foo')\n",
    "        bddat.reset_index(inplace=True)\n",
    "        #bddat.index = pd.DatetimeIndex(bddat['foo'])\n",
    "\n",
    "        data = bddat\n",
    "        del data['foo']\n",
    "        \n",
    "        #data['wateryear'] = data.index.map(wateryear) # calculate the water year\n",
    "        \n",
    "        # calculate accumulated precip and snowfall by water year\n",
    "        \n",
    "        data['ET'] = data.evap+data.trans\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = pd.date_range('2009-10-1','2010-9-30',freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%px import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = params.iloc[60000]\n",
    "\n",
    "#bd = x.basin_daily\n",
    "#patchd = x.patch_daily\n",
    "#idx = x.idx\n",
    "#strt = x.date_peakSWE\n",
    "#nd = x.date_ONS\n",
    "\n",
    "def processSM(bd,idx,strt,nd):\n",
    "    tmp = np.NaN\n",
    "    dr = pd.date_range('2009-10-1','2010-10-02',freq='D')\n",
    "    \n",
    "    lt = len(dr)\n",
    "\n",
    "    # read in the basin daily data\n",
    "    data = pd.read_table(bd, delim_whitespace=True)\n",
    "\n",
    "    if len(data) < lt:\n",
    "        return idx,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp\n",
    "    else:\n",
    "        data.index = pd.DatetimeIndex(dr)\n",
    "        \n",
    "        \n",
    "\n",
    "        data = data.loc[strt:nd] # crop the data to the melt season\n",
    "\n",
    "        data['ET'] = data.evap + data.trans\n",
    "\n",
    "        ET = data.ET.sum() # compute total ET\n",
    "        Q = data.streamflow.sum() # compute total streamflow \n",
    "        Qbf = data.baseflow.sum() # compute total baseflow\n",
    "        Qrf = data['return'].sum() # compute total returnflow,p\n",
    "        sm = data.snowmelt.sum() # compute snowmelt from model\n",
    "        rech = data.recharge.sum() # compute total recharge\n",
    "        sat = data['%sat_area'].mean() # compute the mean saturated area\n",
    "        p = data.precip.sum() # compute water year precip\n",
    "        rz_storage = data.rz_storage.mean() # root zone storage\n",
    "        rz_drainage = data.rz_drainage.mean() # root zone drainage \n",
    "        unsat_drainage = data.unsat_drain.mean() # unsaturated zone drainage\n",
    "        unsat_storage = data.unsat_stor.mean() # unsaturated zone drainage\n",
    "        sat_def = data.sat_def.mean() # saturation deficite (mm)\n",
    "        sat_def_z = data.sat_def_z.mean() # saturation deficite depth, is this the depth to ground water?\n",
    "\n",
    "    # read in the patch data\n",
    "\n",
    "    #data = pd.read_table(patchd, delim_whitespace=True) # load the patch data\n",
    "    \n",
    "    #if len(data) < lt:\n",
    "    #    return idx,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp,tmp\n",
    "    #else:\n",
    "    #    data.index = pd.DatetimeIndex(dr)\n",
    "    #    data = data.loc[strt:nd] # crop the data to the melt season\n",
    "#\n",
    "#        Qin = data.Qin.mean() # saturated flow in\n",
    "#        Qout = data.Qout.mean() # saturated flow out\n",
    "#        streamflow = data.streamflow.mean() # streamflow\n",
    "#        rz_field_capacity = data.rz_field_capacity.mean() # mean field capacity\n",
    "#        snow_melt = data.snow_melt.sum()\n",
    "    \n",
    "        return idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,sat_def,sat_def_z,sm\n",
    "    #,Qin,Qout,streamflow,rz_field_capacity,snow_melt\n",
    "\n",
    "\n",
    "#print idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,sat_def,sat_def_z,idx,Qin,Qout,streamflow,rz_field_capacity,snow_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%px dr = pd.date_range('2009-10-1','2010-9-30',freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processSM(bd,idx,strt,nd):\n",
    "\n",
    "    # read in the basin daily data\n",
    "    data = pd.read_table(bd,delimiter=' ')\n",
    "\n",
    "    data.index = pd.DatetimeIndex(dr)\n",
    "\n",
    "    data['ET'] = data.evap + data.trans\n",
    "\n",
    "    ET = data.ET.sum() # compute total ET\n",
    "    Q = data.streamflow.sum() # compute total streamflow \n",
    "    Qbf = data.baseflow.sum() # compute total baseflow\n",
    "    Qrf = data['return'].sum() # compute total returnflow,p\n",
    "    sm = data.snowmelt.sum()\n",
    "    rech = data.recharge.sum() # compute total recharge\n",
    "    sat = data['%sat_area'].mean() # compute the mean saturated area\n",
    "    p = data.precip.sum() # compute water year precip\n",
    "    rz_storage = data.rz_storage.mean() # root zone storage\n",
    "    rz_drainage = data.rz_drainage.mean() # root zone drainage \n",
    "    unsat_drainage = data.unsat_drain.mean() # unsaturated zone drainage\n",
    "    unsat_storage = data.unsat_stor.mean() # unsaturated zone drainage\n",
    "    sat_def = data.sat_def.mean() # saturation deficite (mm)\n",
    "    sat_def_z = data.sat_def_z.mean() # saturation deficite depth, is this the depth to ground water?\n",
    "    gwStore = data['gw.storage'].sum() # gw store\n",
    "    gwFlux = data['gw.Qout'].mean()\n",
    "    ETRate = data.ET.mean()\n",
    "    \n",
    "    # create full output object\n",
    "    full = (idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,\n",
    "            unsat_storage,unsat_drainage,sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate)\n",
    "    \n",
    "    data = data.loc[strt:nd] # crop the data to the melt season\n",
    "    \n",
    "    ET = data.ET.sum() # compute total ET\n",
    "    Q = data.streamflow.sum() # compute total streamflow \n",
    "    Qbf = data.baseflow.sum() # compute total baseflow\n",
    "    Qrf = data['return'].sum() # compute total returnflow,p\n",
    "    sm = data.snowmelt.sum()\n",
    "    rech = data.recharge.sum() # compute total recharge\n",
    "    sat = data['%sat_area'].mean() # compute the mean saturated area\n",
    "    p = data.precip.sum() # compute water year precip\n",
    "    rz_storage = data.rz_storage.mean() # root zone storage\n",
    "    rz_drainage = data.rz_drainage.mean() # root zone drainage \n",
    "    unsat_drainage = data.unsat_drain.mean() # unsaturated zone drainage\n",
    "    unsat_storage = data.unsat_stor.mean() # unsaturated zone drainage\n",
    "    sat_def = data.sat_def.mean() # saturation deficite (mm)\n",
    "    sat_def_z = data.sat_def_z.mean() # saturation deficite depth, is this the depth to ground water?\n",
    "    gwStore = data['gw.storage'].sum() # gw store\n",
    "    gwFlux = data['gw.Qout'].mean()\n",
    "    ETRate = data.ET.mean()\n",
    "    \n",
    "    melt = (idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,\n",
    "            unsat_storage,unsat_drainage,sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate)\n",
    "    \n",
    "    return full,melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/data/RHESSys_out/Nwt/smex7_test/smex7_test_2_basin.daily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2,\n",
       "  449.7278710000001,\n",
       "  207.78034800000015,\n",
       "  207.78034800000015,\n",
       "  0.0,\n",
       "  252.9876059999999,\n",
       "  0.0,\n",
       "  662.319009,\n",
       "  1541.4915468904094,\n",
       "  0.0,\n",
       "  1165.197779999992,\n",
       "  0.0,\n",
       "  5478.913440000021,\n",
       "  11441.0,\n",
       "  254.863824,\n",
       "  3785.8371879999995,\n",
       "  0.5692612273972607,\n",
       "  1.2321311534246577),\n",
       " (2,\n",
       "  17.496865,\n",
       "  7.0646759999999995,\n",
       "  7.0646759999999995,\n",
       "  0.0,\n",
       "  53.065095,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1547.9127248421053,\n",
       "  0.0,\n",
       "  1165.19778,\n",
       "  0.0,\n",
       "  5478.913440000002,\n",
       "  11441.0,\n",
       "  96.657731,\n",
       "  165.247478,\n",
       "  0.37182505263157895,\n",
       "  0.9208876315789474))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "print params.basin_daily[idx]\n",
    "processSM(params.basin_daily[idx],params.idx[idx],params.date_peakSWE[idx],params.date_ONS[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res = view.map(processSM,params.basin_daily[0:1000],params.idx[0:1000],params.date_peakSWE[0:1000],params.date_ONS[0:1000])\n",
    "res = view.map(processSM,params.basin_daily,params.idx,params.date_peakSWE,params.date_ONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prog = pb.ProgressBar(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*****************99%******************]  49464 of 50000 complete\n"
     ]
    }
   ],
   "source": [
    "while res.ready() == False:\n",
    "    prog.animate_ipython(res.progress)\n",
    "    time.sleep(2)\n",
    "\n",
    "alert.send_alert('barnhatb@colorado.edu','Processing Nr1 %s has finished'%exname,'Your script has finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full,melt = zip(*res.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge processing results with parameters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate = zip(*full)\n",
    "\n",
    "params = pd.read_pickle('./data/params_%s_smex_method%s.pcl'%(site,method)) # load parameters dataset\n",
    "\n",
    "# make a date frame of the results\n",
    "proc = pd.DataFrame()\n",
    "\n",
    "arrs = [idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,\n",
    "        sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate]\n",
    "keys = ['idx','ET','Q','Qbf','Qrf','rech','sat','p','rz_storage','rz_drainage',\n",
    "        'unsat_storage','unsat_drainage','sat_def','sat_def_z','sm','gwStore','gwFlux','ETRate']\n",
    "\n",
    "for arr,key in zip(arrs,keys):\n",
    "    proc[key] = arr # insert the results\n",
    "\n",
    "params = pd.merge(params,proc,on='idx') # join on file index number\n",
    "params.dropna(inplace=True) # drop missing values\n",
    "\n",
    "params.to_hdf('./data/%s_%s_long_full_test.hdf'%(site,exname),'df',format='fixed',complevel=5,complib='bzip2',fletcher32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate = zip(*melt)\n",
    "\n",
    "params = pd.read_pickle('./data/params_%s_smex_method%s.pcl'%(site,method)) # load parameters dataset\n",
    "\n",
    "# make a date frame of the results\n",
    "proc = pd.DataFrame()\n",
    "\n",
    "arrs = [idx,ET,Q,Qbf,Qrf,rech,sat,p,rz_storage,rz_drainage,unsat_storage,unsat_drainage,\n",
    "        sat_def,sat_def_z,sm,gwStore,gwFlux,ETRate]\n",
    "keys = ['idx','ET','Q','Qbf','Qrf','rech','sat','p','rz_storage','rz_drainage',\n",
    "        'unsat_storage','unsat_drainage','sat_def','sat_def_z','sm','gwStore','gwFlux','ETRate']\n",
    "\n",
    "for arr,key in zip(arrs,keys):\n",
    "    proc[key] = arr # insert the results\n",
    "\n",
    "params = pd.merge(params,proc,on='idx') # join on file index number\n",
    "params.dropna(inplace=True) # drop missing values\n",
    "\n",
    "params.to_hdf('./data/%s_%s_long_melt_test.hdf'%(site,exname),'df',format='fixed',complevel=5,complib='bzip2',fletcher32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
